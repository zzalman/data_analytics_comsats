{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxZeMbHNxWJB"
      },
      "outputs": [],
      "source": [
        "# Lab: Data Merging and Joining with Pandas (3 Hours)\n",
        "\n",
        "## Objective:\n",
        "# - Learn how to merge, join, and concatenate datasets using pandas.\n",
        "# - Understand different types of joins (inner, left, right, outer).\n",
        "# - Handle key conflicts, missing values, and track merge origins.\n",
        "# - Apply merging concepts to a real-world case study.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "#Load two dataset student-mat and student-por as math_df and port_df\n",
        "math_df = pd.read_csv(\"/content/student-mat.csv\")\n"
      ],
      "metadata": {
        "id": "_w9-QKp28WkF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "552c9290-cb70-408c-c0cb-4b98b14fbb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xe3 in position 14: invalid continuation byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-40851826232c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Load two dataset student-mat and student-por as math_df and port_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmath_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/student-mat.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe3 in position 14: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the the head\n",
        "\n",
        "#display(port_df.head())"
      ],
      "metadata": {
        "id": "ndkZPzvM8nk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is head in the right format. If not why not?**"
      ],
      "metadata": {
        "id": "5hgHHfIZT_bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z7fmB1_r9Dz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#load data in correct format\n",
        "\n",
        "\n",
        "math_df = pd.read_csv(\"student-mat.csv\", delimiter=\";\") # Specify the delimiter\n",
        "port_df = pd.read_csv(\"student-por.csv\", delimiter=\";\") # Specify the delimiter\n",
        "math_df.head()\n",
        "#display(port_df.head())\n"
      ],
      "metadata": {
        "id": "JTAHP3eD7_uA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "8902f55b-421e-4ee0-df48-bcd84873422c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xe3 in position 14: invalid continuation byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1152e8d030fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmath_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"student-mat.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Specify the delimiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mport_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"student-por.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Specify the delimiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmath_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe3 in position 14: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display (math_df.head())\n",
        "display(port_df.head())\n",
        "\n",
        "# there is a difference between math_df.head() and display(math_df.head()), especially in Jupyter notebooks or environments that support rich output rendering like Google Colab or JupyterLab.\n",
        "\n",
        "#1. math_df.head() This returns the first 5 rows of the DataFrame.\n",
        "\n",
        "# In Jupyter notebooks, when it's the last line in a cell, it will be rendered using rich HTML formatting.\n",
        "\n",
        "# If it's not the last line in a cell, the output might not be shown unless explicitly printed or displayed.\n",
        "\n",
        "# 2. display(math_df.head())\n",
        "# The display() function (from IPython) explicitly tells the notebook to render the DataFrame, using rich formatting regardless of where it appears in the cell.\n",
        "\n",
        "# This is useful when you want to show multiple outputs in the same cell or control exactly when something is displayed."
      ],
      "metadata": {
        "id": "LQHJxCxn9i9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Display the column names of math_df\n",
        "print(\"Columns in math_df:\")\n",
        "print(math_df.columns.tolist())\n",
        "\n",
        "# Display the column names of port_df\n",
        "print(\"\\nColumns in port_df:\")\n",
        "print(port_df.columns.tolist())\n",
        "\n",
        "#The .tolist() method in Python (used with NumPy arrays or Pandas Series/DataFrames) converts the data structure into a regular Python list.\n"
      ],
      "metadata": {
        "id": "fsEsxqQ8_EjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actity 1 : Merge"
      ],
      "metadata": {
        "id": "4lJR42iZ6NV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 1: Merge on common columns (inner join)\n",
        "\n",
        "merged_df = pd.merge(math_df, port_df, on=[\"school\", \"sex\", \"age\"], how=\"inner\", indicator=True)\n",
        "print(merged_df.shape)\n",
        "print(merged_df['_merge'].value_counts())\n",
        "\n",
        "#Merges math_df and port_df using an inner join.\n",
        "\n",
        "#The merge is based on matching values in the columns: \"school\", \"sex\", and \"age\".\n",
        "\n",
        "#how=\"inner\" means only rows with matching values in both dataframes will be included in the result.\n",
        "\n",
        "#indicator=True adds a special column _merge to show the origin of each row:\n",
        "\n",
        "#'left_only' → only in math_df\n",
        "\n",
        "#'right_only' → only in port_df\n",
        "\n",
        "#'both' → matched in both\n"
      ],
      "metadata": {
        "id": "EOBzWfEo6GCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.shape)"
      ],
      "metadata": {
        "id": "oDyt48q6BCyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(merged_df.head())\n"
      ],
      "metadata": {
        "id": "0NngTTHTCw1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#1. **`inner join` on `school`, `sex`, and `age`:**  An inner join only includes\n",
        "#rows where the values of *all* specified join keys (`school`, `sex`, and `age`)\n",
        "# match in *both* dataframes.  If a student exists in `math_df` but has a different\n",
        "# value for any of these keys in `port_df`, or vice-versa, that student's record will be excluded from the merged result.\n",
        "\n",
        "#2. **Column name conflicts:**  Because several columns share names between the two dataframes\n",
        "# (e.g., `absences`, `G1`, `G2`, `G3`), Pandas appends `_x` to columns from the left dataframe (`math_df`) and `_y` to columns from the right dataframe (`port_df`) when merging to create unique column names.  This is what you are seeing with `absences_y` and `absences_x`. This means `absences_y` refers to the `absences` value from `port_df`.\n",
        "\n",
        "\n",
        "#3. **First five rows may not match:**  While the first few rows of `math_df` and\n",
        "#`port_df` might seem to have similar values for `school`, `sex`, and `age`, the\n",
        "#crucial point is that an inner join requires *exact* matches across *all three* columns. A slight difference in one of those columns (even in a row not shown by `.head()`) for a given student in either dataframe, will prevent that student's data from being included in the merged output. The merged DataFrame's first five rows are the first five rows where the merge keys fully match between both dataframes.\n",
        "\n",
        "\n",
        "#**In summary:** The `merged_df` displays different values for `absences`\n",
        " #(and other duplicated columns) because it's showing the value of `absences`\n",
        " #from `port_df` (`absences_y`). The first five rows of the merged dataset do\n",
        " # not correspond directly to the first five rows of the original dataframes because of the `inner join` logic which filters out rows that don't have exact matches on all the specified merge columns.  The resulting `merged_df` only shows the rows that have a perfect match for `school`, `sex`, and `age` in both `math_df` and `port_df`.\n"
      ],
      "metadata": {
        "id": "UJ7RmP4mDyK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Merge with different column names\n",
        "port_df_renamed = port_df.rename(columns={\"age\": \"student_age\"})\n",
        "merged_custom = pd.merge(math_df, port_df_renamed, left_on=\"age\", right_on=\"student_age\", how=\"inner\")\n",
        "print(merged_custom.head())\n",
        "\n",
        "#Merges math_df and the renamed port_df_renamed.\n",
        "\n",
        "#left_on=\"age\": Use the age column from math_df.\n",
        "\n",
        "#right_on=\"student_age\": Use the student_age column from port_df_renamed.\n",
        "\n",
        "#how=\"inner\": Only rows where both age and student_age match will be included.\n",
        "#Sometimes, datasets use different naming conventions even for the same data.\n",
        "#This technique lets you merge them by explicitly matching the relevant columns.\n",
        "#examples \"age\": \"student_age\",     \"school\": \"school_name\",     \"sex\": \"gender\""
      ],
      "metadata": {
        "id": "opFNL62C-18O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show column names of merged_custom.df\n",
        "\n",
        "print(merged_custom.columns.tolist())\n"
      ],
      "metadata": {
        "id": "A7DB51UGKStk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 3: Observe effect of duplicate keys\n",
        "math_dup = math_df.iloc[:10].copy()\n",
        "math_dup = pd.concat([math_dup, math_dup], ignore_index=True)\n",
        "merged_dup = pd.merge(math_dup, port_df, on=[\"school\", \"sex\", \"age\"], how=\"inner\")\n",
        "print(merged_dup.shape)  # many-to-many merge"
      ],
      "metadata": {
        "id": "86uu60TGLT72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: briefly explain what does this command do \"# Task 3: Observe effect of duplicate keys\n",
        "# math_dup = math_df.iloc[:10].copy()\n",
        "# math_dup = pd.concat([math_dup, math_dup], ignore_index=True)\n",
        "# merged_dup = pd.merge(math_dup, port_df, on=[\"school\", \"sex\", \"age\"], how=\"inner\")\n",
        "# print(merged_dup.shape)  # many-to-many merge\"\n",
        "\n",
        "# This code demonstrates the effect of duplicate keys on a many-to-many merge operation using Pandas.\n",
        "\n",
        "#1. **Duplicate Creation:** It takes the first 10 rows of the `math_df` DataFrame and creates a copy called `math_dup`. Then, it concatenates `math_dup` with itself, effectively doubling the rows and creating duplicate entries.  The `ignore_index=True` argument resets the index of the concatenated DataFrame.\n",
        "\n",
        "#2. **Many-to-Many Merge:** It then performs an inner merge between the modified `math_dup` (which now has duplicate rows) and the `port_df` DataFrame based on the columns \"school\", \"sex\", and \"age\". Because `math_dup` has duplicates and `port_df` may also have matching rows on the join columns, this creates a many-to-many merge scenario.  For each duplicate row in `math_dup` that finds a match in `port_df`, a new row will be created in the `merged_dup` DataFrame.\n",
        "\n",
        "#3. **Shape Observation:**  Finally, it prints the shape (number of rows and columns) of the resulting `merged_dup` DataFrame.  The shape will be larger than the shape of `port_df` because of the duplication in `math_dup`. The number of rows in the `merged_dup` DataFrame would be equal to the number of times each duplicated row in `math_dup` is matched in `port_df`.\n",
        "\n",
        "#In essence, this section illustrates how duplicate keys in a merge operation\n",
        "#can lead to an increase in the number of rows in the resulting merged DataFrame, reflecting a many-to-many relationship between the dataframes being joined.\n"
      ],
      "metadata": {
        "id": "u8zrAtRcLghU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Lab Activity 2: Join Types\n",
        "# ---------------------------\n",
        "\n",
        "# Create sample HR datasets\n",
        "#https://github.com/pouyasattari/HR-Dataset-Analysis/blob/main/HRDataset_v14.csv\n",
        "hr_df = pd.read_csv(\"/content/HRDataset_v14.csv\")"
      ],
      "metadata": {
        "id": "HJ9TqFT_MUz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "departments = hr_df[['Department', 'Employee_Name']].drop_duplicates()\n",
        "training_scores = pd.DataFrame({\n",
        "    'Employee_Name': ['Douglas', 'Katelyn', 'John', 'Zoe'],\n",
        "    'Training_Score': [88, 92, 75, 69]\n",
        "})\n",
        "#hr_df[['Department', 'Employee_Name']]\n",
        "# Selects just the Department and Employee_Name columns from the HR dataset.\n",
        "\n",
        "#.drop_duplicates()\n",
        "# Removes duplicate rows where the same employee appears more than once in\n",
        "#the same department.\n",
        "\n",
        "#Stores result in departments\n",
        "# Now you have a simplified DataFrame showing unique employee-department\n",
        "#pairs.\n",
        "\n",
        "#--------------\n",
        "#training_scores = pd.DataFrame({    'Employee_Name': ['Douglas', 'Katelyn', 'John', 'Zoe'],'Training_Score': [88, 92, 75, 69]})\n",
        "#This creates a new DataFrame manually containing training scores for a few employees.\n",
        "\n",
        "#You're preparing two datasets that can be merged later.\n",
        "\n",
        "#One shows which department each employee belongs to.\n",
        "\n",
        "#The other shows each employee's training score.\n"
      ],
      "metadata": {
        "id": "xx2BQMS_OCxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hr_df.head()"
      ],
      "metadata": {
        "id": "8G6N_FcSONNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show top 5 rows of the departments\n"
      ],
      "metadata": {
        "id": "OEa6Z815Of9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show head of the training scores"
      ],
      "metadata": {
        "id": "EbkiEtlGOqK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Inner Join\n",
        "inner_joined = pd.merge(hr_df, training_scores, on=\"Employee_Name\", how=\"inner\")\n",
        "\n",
        "#This command performs an inner join between hr_df and training_scores on the Employee_Name column, keeping only the rows where the employee exists in both DataFrames."
      ],
      "metadata": {
        "id": "BxY6m3FiQXdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 2: Left Join with NaN handling\n",
        "left_joined = pd.merge(hr_df, training_scores, on=\"Employee_Name\", how=\"left\")\n",
        "left_joined['Training_Score'] = left_joined['Training_Score'].fillna(0)\n",
        "\n",
        "#Perform a left join between hr_df and training_scores on Employee_Name,\n",
        "#keeping all records from hr_df and adding Training_Score where available.\n",
        "\n",
        "#Replace missing (NaN) training scores with 0 to handle unmatched employees\n",
        "#who didn’t have a training score.\n"
      ],
      "metadata": {
        "id": "dSX4edPHUuuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "left_joined.head()"
      ],
      "metadata": {
        "id": "0ixjF7O_U1Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 3: Outer Join\n",
        "outer_joined = pd.merge(hr_df, training_scores, on=\"Employee_Name\", how=\"outer\", indicator=True)\n",
        "print(outer_joined['_merge'].value_counts())\n",
        "\n",
        "#Perform an outer join between hr_df and training_scores on Employee_Name,\n",
        "#keeping all records from both DataFrames whether they match or not.\n",
        "#Count and print how many rows came from both DataFrames (both),\n",
        "#only from hr_df (left_only), or only from training_scores (right_only) using the _merge indicator column."
      ],
      "metadata": {
        "id": "nOPv19BxVgLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------\n",
        "# Lab Activity 3: Case Study\n",
        "# ---------------------------\n",
        "\n",
        "# Case: Unify HR demographics, departments, and training into a clean profile\n",
        "employee_df = hr_df[['Employee_Name', 'Department', 'Position', 'Sex', 'MaritalDesc']]\n",
        "final_profile = pd.merge(employee_df, training_scores, on=\"Employee_Name\", how=\"left\")"
      ],
      "metadata": {
        "id": "8dZs6KjVWMFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Clean up missing scores\n",
        "final_profile['Training_Score'] = final_profile['Training_Score'].fillna(\"Not Participated\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TNMfJExoIgWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show result\n",
        "print(final_profile.head())\n"
      ],
      "metadata": {
        "id": "W5_jOlXcWnwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practice Activities\n",
        "\n",
        "# Activity 1: Exploring Different Merge Types\n",
        "\n",
        "# Objective:  Practice merging two datasets using various join types (inner, left, right, outer) and observe the differences in the resulting dataframes.\n",
        "\n",
        "# Instructions:\n",
        "# 1. Load the \"student-mat.csv\" and \"student-por.csv\" datasets into pandas DataFrames (remember to specify the delimiter ';').\n",
        "# 2. Merge the two dataframes using each of the four merge types (inner, left, right, outer) on the common columns \"school\", \"sex\", and \"age\".\n",
        "# 3. For each merged dataframe, observe:\n",
        "#    - The number of rows and columns.\n",
        "#    - The content of the first few rows.\n",
        "#    - The unique values and counts in the '_merge' indicator column (if used).  How do these counts explain the rows in the merged data?\n",
        "# 4. Explain the differences between the merge types based on your observations.\n",
        "# 5. Answer the following questions:\n",
        "#     - Which merge type is most suitable for this scenario where you're combining academic data from two sources?\n",
        "#     - Which merge type preserves information from only one dataset, and which one from both?  How is \"information\" preserved in a merge?  Is it a row or a column or something else?\n",
        "#     - How does using `indicator=True` help you understand the merge results?\n",
        "# 6. Submit your code and analysis (explanations) in a Jupyter Notebook.\n",
        "\n",
        "# Activity 2: Handling Missing Values and Duplicate Keys\n",
        "\n",
        "# Objective:  Practice merging datasets with missing values and duplicate keys and apply appropriate strategies for handling these situations.\n",
        "\n",
        "# Instructions:\n",
        "# 1. Use the provided HR datasets (`hr_df`, `departments`, and `training_scores`) or create similar datasets with some missing values in the `Training_Score` column and duplicate employee names.\n",
        "# 2. Perform a left join between `hr_df` and `training_scores` on \"Employee_Name\" with missing values filled (NaNs). Experiment with different fill methods.\n",
        "# 3. Create duplicate rows within one of the dataframes that are merged. Perform the same left join. What changes in your outcome? How many rows are in each merge outcome?\n",
        "# 4. Perform an inner join between the same two dataframes. Note how many rows are returned. Compare the output of the inner join to the output of the left join with missing values filled. Why might the inner join output a different number of rows?\n",
        "# 5. Explore the effect of using different join keys.\n",
        "# 6.  What are the trade-offs between using an inner join versus a left/right join?\n",
        "# 7. Submit your code and observations/explanations in a Jupyter Notebook.\n"
      ],
      "metadata": {
        "id": "Hl4SbCdXW7sz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}